\documentclass{article}         %% What type of document you're writing.
\usepackage[fleqn]{amsmath}
\usepackage{amsfonts,amssymb}   %% AMS mathematics macros
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amssymb}
\usepackage{parskip}
\usepackage{mathtools}
\usepackage{bm}
\usepackage[margin=1.3in]{geometry}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{tikz}
%\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}
\usetikzlibrary{backgrounds}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]
\newcommand\descitem[1]{\item{\bfseries #1}}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\re}{Re}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\argmax}{\arg\max}
\DeclareMathOperator{\pad}{pad}
\graphicspath{ {./figs/} }
\newcommand{\inv}{^{-1}}
\newcommand{\pd}{\partial}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\mat}[1]{\begin{matrix} #1 \end{matrix}}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\norm}[1]{\left|\left| #1 \right|\right|}
\newcommand{\cb}[1]{\left\{ #1 \right\}}
\newcommand{\pn}[1]{\left( #1 \right)}
\newcommand{\bc}[1]{\left[ #1 \right]}
\renewcommand{\cal}[1]{\mathcal{#1}}
\newcommand{\eps}{\varepsilon}
\DeclareMathOperator{\Tr}{Tr}
\usepackage{titling}

\title{Shape normalization for keypoint data}
\author{Caleb Weinreb and Kai Fox}
\date{May 2023}

\begin{document}

\maketitle

\section{Modeling framework}
\label{sec:model-fwk}

The goal of shape normalization is to map keypoint observations from many animals into a standardized pose space where the effects of body morphology have been removed. Naively, one might hope to normalize using simple summary statistics like average nose-to-tail distance. But these statistics are problematic because they conflate body shape with behavior. For example, larger average nose-to-tail distance could reflect a larger body, or simply a higher frequency of stretched-out poses. Thus the central challenge of shape normalization is to isolate the effects of behavior and body shape so that the latter can be cleanly removed. To that end, we propose a generative model that factors behavior and morphology into separate terms that are disentangled during fitting.

\subsection{Input data}

Input data consist of keypoints from $N$ animals measured at $T_n$ timepoints (the timepoints need not be sequential). Before modeling, the keypoints are preprocessed by (1) centering and rotational alignment; (2) uniform scaling based on average inter-keypoint distances; (3) low-dimensional embedding using principal components analysis (denote the embedding dimension as $D$). Together, these steps yield a set of low-dimensional poses $y_{n,t} \in \mathbb{R}^D$ for animals $n$ and timepoints $t$. 

\subsection{Generative model}
\label{sec:gen-model}

We model each pose $y_{n,t}$ as the animal-specific realization of a standardized pose $x_{n,t}$ that is sampled from a Gaussian mixture. The mixture components are shared across animals, but the mixture weights are animal-specific. Formally, the model is
%
\begin{align*}
    y_{n,t} & = f(x_{n,t}, \phi_n) 
        && \text{(observed pose given standardized pose)} \\
    x_{n,t} & \sim \NN(m_{z_{n,t}}, Q_{z_{n,t}}) 
        && \text{(standardized pose given mixture component)}  \\
    z_{n,t} & \sim \text{Cat}(\pi_n)  
        && \text{(mixture component given animal-specific weights)} \\
    m_z & \sim \NN(m_0, \lambda_0^{-1} Q_z) 
        && \text{(prior on mixture means given covariances)} \\
    Q_z & \sim \text{Wishart}^{-1}(W_0, \nu_0) 
        && \text{(prior on mixture covariances)} \\
    \pi_n & \sim \text{Dir}(\beta)
        && \text{(prior on animal-specific mixture weights)} \\
    \beta & \sim \text{Dir}(\gamma,\dots,\gamma)
        && \text{(prior on mixture weight hyperparameters)}
\end{align*}   
%
where $f$ is a morph function that maps standardized poses $x_{n,t}$ to animal-specific poses $y_{n,t}$ and $\phi_n$ is a parameter vector that captures the unique shape of animal $n$. The specific morph function used in this paper is described below. In principle, any morph function $f$ could be used as long as it is invertible in its first argument and differentiable in its second. Abusing notation, we will use $f^{-1}$ to denote the inverse of $f$ in its first argument.


\subsection{Morph function} \label{morph}

We use an affine transformation for the morph $f$. To avoid degeneracy and promote interpretability, one animal (say $n=0$) is defined as a ``reference'' and its morph is fixed as the identity. Morphs for the remaining animals can then be thought of as perturbations of the reference. For clarity, these perturbations are written below as the combination (1) an additive perturbation to the reference animal's mean pose and (2) a linear perturbation to residuals around the mean pose. To limit degrees of freedom (and thus prevent over-fitting), these perturbations are restricted to the top $L$ principal components of pose space (where $L \le D$). The bottom $D-L$ components are preserved without modification. Thus, for each animal $n > 0$, the morph parameters $\phi_n$ consist of a matrix $U_n \in \mathbb{R}^{L \times L}$ and a vector $v_n \in \mathbb{R}^L$ that act on poses $x \in \mathbb{R}^D$ as follows:
%
\begin{align}
    f(x, (U_n, v_n)): x \mapsto 
    \begin{bmatrix} U_n & \mathbf{0} \\ \mathbf{0} & I_{D-L} \end{bmatrix}
    (x - \bar{x}) + 
    \begin{bmatrix} v_n & \mathbf{0}_{D-L}\end{bmatrix} + \bar{x}
\end{align}
%
where $\bar{x}$ is the mean pose of the refernece animal.

\section{Inference}

\subsection{Expectation maximization}
\label{sec:em-general}

The model parameters $\theta = (\phi, m, Q, \pi, \beta)$ are fit using expectation maximization (EM), which aims to maximize the expected log likelihood $\ell(\theta) = \EE \log P(y, z \mid \theta)$. EM alternates between two steps.


% EM iteratively computes a posterior $q(z) = P(z \mid y, \theta^*)$ given the current parameter estimates $\theta^*$ (E-step), and then finds new parameter estimates $\theta^*_\text{new}$ by maximizing the following objective with respect to $\theta$ (M-step).
% \begin{align}
%     A(\theta; \theta^*) = \EE_{q(z)} \log P(y, z \mid \theta) + \log P(\theta)
%     \label{eq:A-defn}
% \end{align}
% The theoretical basis for EM is that this procedure guarantees monotonic increases in the expected log likelihood, i.e. $\ell(\theta^*_{\text{new}}) \geq \ell(\theta^*)$ (see Murphy 11.4.7 [cite]). Expressions for $q(z)$ and $A(\theta; \theta^*)$ are given below. We perform the M-step optimization with gradient ascent. 

\paragraph{E-step} Given current parameter estimates $\theta^* = (\phi^*, m^*, Q^*, \pi^*, \beta^*)$, calculate the posterior $q(z) = P(z \mid y, \theta^*)$, as follows.
%
\begin{align}
    q(z) & = \prod_{n,t} P(z_{n,t} \mid y_{n,t}, \phi^*, m^*, Q^*, \pi^*) \\
    & \propto  \prod_{n,t}  \NN(f^{-1}(y_{n,t}, \phi^*_n) \mid m_{z_{n,t}^*}, Q_{z_{n,t}^*}) \cdot \pi_{n,z_{n,t}}^* \label{eq:q-propto-statement}
\end{align}

\paragraph{M-step} Given $q(z)$ from the E-step, obtain new parameter estimates by optimizing the following objective with respect to $\theta = (\phi, m, Q, \pi, \beta)$ (using gradient ascent).
\begin{align}
    & A(\theta; \theta^*)  = \sum_{n,t} \EE_q \log P(y_{n,t}, z_{n,t} \mid \theta) + \log P(\theta) \\
    & =  \sum_{n,t} q(z_{n,t}) \log \left[ \NN(f^{-1}(y_{n,t}, \phi_n) \mid m_{z_{n,t}}, Q_{z_{n,t}}) \cdot \det(J)  \cdot \pi_{n, z_{n, t}} \right] + \log P(\theta) \label{eq:A-modeled}
\end{align}
where $J$ is the Jacobian of $f^{-1}$ (in its first argument) evaluated at $y_{n,t}$.

\subsection{Initialization}
Initialization of the morph parameters $\phi$ is described in section XXX. Given morph parameters, we fit a standard Gaussian mixture model to the points $x_{n,t} = f^{-1}(y_{n,t}, \phi_n)$ (e.g. with sklearn), yielding mixture components $(m_z, Q_z), z=1,...,L$ and cluster weights $\lambda_{n,t,z}$ for each data point. Using the weights, we initialize $\pi$ and $\beta$ as follows.
\begin{align}
    \pi_{n,z} = \frac{1}{T_n} \sum_{t=1}^{T_n} \lambda_{n,t,z}, \quad \beta_z = \frac{1}{N} \sum_{n=1}^N \pi_{n,z}
\end{align}



\subsection{E-step details}

In the E-step we seek to calculate $q_{n, t}(z) = P(z\mid y_{n, t},\,\theta^*)$ for a given keypoint observation $y_{n, t}$ and estimated parameters $\theta^*$. This is usually done using Bayes' rule
\begin{align}
    q_{n,t}(z) = P(z\mid y_{n, t}, \theta^*) = \frac{P(y_{n, t}\mid z,\,\theta^*)P(z\mid \theta^*)}{\sum_{z'} P(y_{n, t}\mid z',\,\theta^*)P(z'\mid \theta^*)}
\end{align}
Each probability above is given directly by the generative model (Sec \ref{sec:gen-model}), and so may be expanded as
\begin{align}
    q_{n,t}(z) = \frac{ \NN(f^{-1}(y_{n,t}, \phi^*_n) \mid m_{z}^*, Q_{z}^*) \cdot \pi_{n,z}^* }{ \sum_{z'} \NN(f^{-1}(y_{n,t}, \phi^*_n) \mid m_{z'}^*, Q_{z'}^*) \cdot \pi_{n,z'}^* }
\end{align}
which yields the proportionality result stated in Eq. \ref{eq:q-propto-statement} with the additional specification of normalization so that $q_{n,t}$ is a probability distribution in $z$.


\end{document}